{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7910fb1",
   "metadata": {},
   "source": [
    "# [1] Load Experiment Configs\n",
    "\n",
    "Change the configs in `run_config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e341fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Run config file: run_config.json==========\n",
      "[2] datasets              : ['ETTh1', 'ETTh2']\n",
      "[6] models                : ['DLinear', 'PatchTST', 'TimesNet', 'PAttn', 'TimeMixer', 'TimeXer']\n",
      "[1] forecast_settings     : [[96, 48, 96]]\n",
      "override_args             : {'use_gpu': True, 'gpu': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ETTh1_DLinear_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='DLinear', data='ETTh1', root_path='./dataset/long_term_forecast/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh1', n_dim=7),\n",
       " 'ETTh1_PatchTST_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='PatchTST', data='ETTh1', root_path='./dataset/long_term_forecast/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=2, e_layers=1, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh1', n_dim=7),\n",
       " 'ETTh1_TimesNet_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='TimesNet', data='ETTh1', root_path='./dataset/long_term_forecast/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh1', n_dim=7),\n",
       " 'ETTh1_PAttn_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='PAttn', data='ETTh1', root_path='./dataset/long_term_forecast/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=2, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh1', n_dim=7),\n",
       " 'ETTh1_TimeMixer_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='TimeMixer', data='ETTh1', root_path='./dataset/long_term_forecast/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=128, patience=10, learning_rate=0.01, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh1', n_dim=7),\n",
       " 'ETTh1_TimeXer_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='TimeXer', data='ETTh1', root_path='./dataset/long_term_forecast/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=256, n_heads=8, e_layers=1, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=4, patience=3, learning_rate=0.0001, des=\"'exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh1', n_dim=7),\n",
       " 'ETTh2_DLinear_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_96_96', model='DLinear', data='ETTh2', root_path='./dataset/long_term_forecast/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh2', n_dim=7),\n",
       " 'ETTh2_PatchTST_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_96_96', model='PatchTST', data='ETTh2', root_path='./dataset/long_term_forecast/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=4, e_layers=3, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh2', n_dim=7),\n",
       " 'ETTh2_TimesNet_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_96_96', model='TimesNet', data='ETTh2', root_path='./dataset/long_term_forecast/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh2', n_dim=7),\n",
       " 'ETTh2_PAttn_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_96_96', model='PAttn', data='ETTh2', root_path='./dataset/long_term_forecast/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh2', n_dim=7),\n",
       " 'ETTh2_TimeMixer_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_96_96', model='TimeMixer', data='ETTh2', root_path='./dataset/long_term_forecast/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.01, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh2', n_dim=7),\n",
       " 'ETTh2_TimeXer_96_48_96': Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh2_96_96', model='TimeXer', data='ETTh2', root_path='./dataset/long_term_forecast/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', inverse=False, mask_rate=0.25, anomaly_ratio=0.25, expand=2, d_conv=4, top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=256, n_heads=8, e_layers=1, d_layers=1, d_ff=1024, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', seg_len=48, num_workers=64, itr=1, train_epochs=10, batch_size=16, patience=3, learning_rate=0.0001, des=\"'Exp'\", loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, gpu_type='cuda', use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, use_dtw=False, augmentation_ratio=0, seed=2, jitter=False, scaling=False, permutation=False, randompermutation=False, magwarp=False, timewarp=False, windowslice=False, windowwarp=False, rotation=False, spawner=False, dtwwarp=False, shapedtwwarp=False, wdba=False, discdtw=False, discsdtw=False, extra_tag='', verbose=True, patch_len=16, data_name='ETTh2', n_dim=7)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_configs import get_all_exp_args, load_run_config\n",
    "\n",
    "# load the TimeFuse exp configs\n",
    "run_configs = load_run_config(\"run_config.json\", verbose=True)\n",
    "\n",
    "# load the base model exp args (used for TSLib models) for base model train and inference\n",
    "all_exp_args = get_all_exp_args(\n",
    "    datasets=run_configs[\"datasets\"],\n",
    "    models=run_configs[\"models\"],\n",
    "    forecast_settings=run_configs[\"forecast_settings\"],\n",
    "    override_args=run_configs[\"override_args\"],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "all_exp_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b9708",
   "metadata": {},
   "source": [
    "# [2] Base Model Training\n",
    "\n",
    "Train base models for each dataset and forecast settin, the trained model will be saved in the `checkpoints` folder.\n",
    "- If the checkpoint folder does not exist, it will be created automatically.\n",
    "- If the checkpoint folder already exists, the training will be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9738de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc41f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device: cuda:0] Base model training 96_48_96/ETTh1_DLinear_dmodel512_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh1_DLinear_dmodel512_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh1_PatchTST_dmodel512_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh1_PatchTST_dmodel512_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh1_TimesNet_dmodel16_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh1_TimesNet_dmodel16_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh1_PAttn_dmodel512_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh1_PAttn_dmodel512_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_0_96/ETTh1_TimeMixer_dmodel16_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_0_96/ETTh1_TimeMixer_dmodel16_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh1_TimeXer_dmodel256_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh1_TimeXer_dmodel256_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh2_DLinear_dmodel32_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh2_DLinear_dmodel32_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh2_PatchTST_dmodel512_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh2_PatchTST_dmodel512_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh2_TimesNet_dmodel32_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh2_TimesNet_dmodel32_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh2_PAttn_dmodel32_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh2_PAttn_dmodel32_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_0_96/ETTh2_TimeMixer_dmodel16_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_0_96/ETTh2_TimeMixer_dmodel16_epoch10 | Set override_saved_model=True to train and override.\n",
      "[Device: cuda:0] Base model training 96_48_96/ETTh2_TimeXer_dmodel256_epoch10\n",
      "[Base Model Train] Model already trained, loading from ./checkpoints/96_48_96/ETTh2_TimeXer_dmodel256_epoch10 | Set override_saved_model=True to train and override.\n"
     ]
    }
   ],
   "source": [
    "from exp.exp_fuse_forecasting import Exp_Fuse_Forecasting\n",
    "\n",
    "for exp_name, args in all_exp_args.items():\n",
    "    setting = \"{}_{}_{}/{}_{}_dmodel{}_epoch{}\".format(\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.data_name,\n",
    "        args.model,\n",
    "        args.d_model,\n",
    "        args.train_epochs,\n",
    "    )\n",
    "    exp = Exp_Fuse_Forecasting(args)\n",
    "    print(f\"[Device: {exp.device}] Base model training {setting}\")\n",
    "    model, vali_loss, test_loss = exp.train(\n",
    "        setting=setting,\n",
    "        verbose=False,\n",
    "        tqdm_disable=False,\n",
    "        save_model=True,\n",
    "        override_saved_model=False,\n",
    "        raise_fwd_error=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e6a40",
   "metadata": {},
   "source": [
    "# [3] Meta-training Data Extraction\n",
    "\n",
    "Extract meta-training data based on the trained base models. The extracted data will be saved in the `meta_data/` folder.\n",
    "\n",
    "Given a training time series pair (`X_in`, `X_out`) and k base models, we extract the following meta-training data:\n",
    "- `x_meta`: the meta-features of the input time series `X_in`\n",
    "- `y_model_preds`: the predictions of the k base models (i.e., their preditions of `X_out`)\n",
    "- `y_true`: the ground truth `X_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7db0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input Meta Feat Extract] File ./meta_data/ETTh1_val/x_meta_96.h5 already exists, skipping...\n",
      "[Output Pred & True Extract] Files ./meta_data/ETTh1_val/y_pred_96_48_96.h5 and ./meta_data/ETTh1_val/y_true_96_48_96.h5 already exist, skipping...\n",
      "[Input Meta Feat Extract] File ./meta_data/ETTh1_test/x_meta_96.h5 already exists, skipping...\n",
      "[Output Pred & True Extract] Files ./meta_data/ETTh1_test/y_pred_96_48_96.h5 and ./meta_data/ETTh1_test/y_true_96_48_96.h5 already exist, skipping...\n",
      "[Input Meta Feat Extract] File ./meta_data/ETTh2_val/x_meta_96.h5 already exists, skipping...\n",
      "[Output Pred & True Extract] Files ./meta_data/ETTh2_val/y_pred_96_48_96.h5 and ./meta_data/ETTh2_val/y_true_96_48_96.h5 already exist, skipping...\n",
      "[Input Meta Feat Extract] File ./meta_data/ETTh2_test/x_meta_96.h5 already exists, skipping...\n",
      "[Output Pred & True Extract] Files ./meta_data/ETTh2_test/y_pred_96_48_96.h5 and ./meta_data/ETTh2_test/y_true_96_48_96.h5 already exist, skipping...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>seq_len_label_len_pred_len</th>\n",
       "      <th>split_name</th>\n",
       "      <th>model_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dataset_name, seq_len_label_len_pred_len, split_name, model_scores]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp.exp_fuse_forecasting import Exp_Fuse_Forecasting\n",
    "from utils.save_array import save_arr\n",
    "from utils.metrics import metric\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_input_meta_feats(\n",
    "    run_configs,\n",
    "    all_exp_args,\n",
    "    meta_train_root,\n",
    "    dataset_name,\n",
    "    split_name,\n",
    "    seq_len,\n",
    "    force_override=False,\n",
    "):\n",
    "    # Extract input temporal meta feature\n",
    "    meta_file_path = (\n",
    "        f\"{meta_train_root}/{dataset_name}_{split_name}/x_meta_{seq_len}.h5\"\n",
    "    )\n",
    "    if not force_override and os.path.exists(meta_file_path):\n",
    "        print(\n",
    "            f\"[Input Meta Feat Extract] File {meta_file_path} already exists, skipping...\"\n",
    "        )\n",
    "        return\n",
    "    else:\n",
    "        print(\n",
    "            f\"[Input Meta Feat Extract] Extracting {dataset_name}-{split_name} meta features...\"\n",
    "        )\n",
    "\n",
    "    args = all_exp_args[\n",
    "        f\"{dataset_name}_{run_configs['models'][0]}_{seq_len}_{label_len}_{pred_len}\"\n",
    "    ]  # for initializing the exp class only\n",
    "    exp = Exp_Fuse_Forecasting(args)\n",
    "    df_x_meta = exp.get_test_meta_feature(\n",
    "        split_name=split_name\n",
    "    )  # extract input time series meta features\n",
    "    save_arr(\n",
    "        arr=df_x_meta.values,\n",
    "        file_path=meta_file_path,\n",
    "        file_type=\"h5\",\n",
    "        create_dir=True,\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "def extract_output_pred_true(\n",
    "    run_configs,\n",
    "    dataset_name,\n",
    "    meta_train_root,\n",
    "    split_name,\n",
    "    seq_len,\n",
    "    label_len,\n",
    "    pred_len,\n",
    "    force_override=False,\n",
    "):\n",
    "    # Extract and save model predictions & ground truth\n",
    "    postfix = f\"{seq_len}_{label_len}_{pred_len}\"\n",
    "    pred_file_path = (\n",
    "        f\"{meta_train_root}/{dataset_name}_{split_name}/y_pred_{postfix}.h5\"\n",
    "    )\n",
    "    true_file_path = (\n",
    "        f\"{meta_train_root}/{dataset_name}_{split_name}/y_true_{postfix}.h5\"\n",
    "    )\n",
    "    if (\n",
    "        not force_override\n",
    "        and os.path.exists(pred_file_path)\n",
    "        and os.path.exists(true_file_path)\n",
    "    ):\n",
    "        print(\n",
    "            f\"[Output Pred & True Extract] Files {pred_file_path} and {true_file_path} already exist, skipping...\"\n",
    "        )\n",
    "        return\n",
    "    else:\n",
    "        print(\n",
    "            f\"[Output Pred & True Extract] Extracting {dataset_name}-{split_name} predictions and ground truth...\"\n",
    "        )\n",
    "\n",
    "    data_preds = {}\n",
    "    for model_name in run_configs[\"models\"]:\n",
    "\n",
    "        args = all_exp_args[\n",
    "            f\"{dataset_name}_{model_name}_{seq_len}_{label_len}_{pred_len}\"\n",
    "        ]\n",
    "        exp = Exp_Fuse_Forecasting(args)\n",
    "\n",
    "        setting = \"{}_{}_{}/{}_{}_dmodel{}_epoch{}\".format(\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.data_name,\n",
    "            args.model,\n",
    "            args.d_model,\n",
    "            args.train_epochs,\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(\n",
    "            f\"[Output Pred & True Extract] Inferencing ({exp.device}): \",\n",
    "            setting,\n",
    "            end=\" ... \\t\",\n",
    "        )\n",
    "        (\n",
    "            preds,\n",
    "            trues,\n",
    "            mae,\n",
    "            mse,\n",
    "            rmse,\n",
    "            mape,\n",
    "            mspe,\n",
    "        ) = exp.test(  # predict with saved model\n",
    "            setting=setting,\n",
    "            split_name=split_name,\n",
    "            load_saved_model=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(f\"Done in {time.time() - start_time:.2f} seconds\")\n",
    "        data_preds[model_name] = preds\n",
    "\n",
    "        del exp\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # rearrange preds dimension\n",
    "    print(f\"[Output Pred & True Extract] Rearranging preds dimension ...\", end=\"\")\n",
    "    start_time = time.time()\n",
    "    all_model_preds = np.array(\n",
    "        [data_preds[model_name] for model_name in run_configs[\"models\"]]\n",
    "    ).transpose(1, 0, 2, 3)\n",
    "    print(f\"Done in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    postfix = f\"{seq_len}_{label_len}_{pred_len}\"\n",
    "    pred_file_path = (\n",
    "        f\"{meta_train_root}/{dataset_name}_{split_name}/y_pred_{postfix}.h5\"\n",
    "    )\n",
    "    true_file_path = (\n",
    "        f\"{meta_train_root}/{dataset_name}_{split_name}/y_true_{postfix}.h5\"\n",
    "    )\n",
    "    save_arr(\n",
    "        arr=all_model_preds,\n",
    "        file_path=pred_file_path,\n",
    "        file_type=\"h5\",\n",
    "        create_dir=True,\n",
    "    )\n",
    "    save_arr(\n",
    "        arr=trues,\n",
    "        file_path=true_file_path,\n",
    "        file_type=\"h5\",\n",
    "        create_dir=True,\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "split_names = [\"val\", \"test\"]\n",
    "meta_train_root = \"./meta_data\"\n",
    "\n",
    "for dataset_name in run_configs[\"datasets\"]:\n",
    "    for seq_len, label_len, pred_len in run_configs[\"forecast_settings\"]:\n",
    "        for split_name in split_names:\n",
    "            # Extract input temporal meta feature\n",
    "            extract_input_meta_feats(\n",
    "                run_configs=run_configs,\n",
    "                all_exp_args=all_exp_args,\n",
    "                meta_train_root=meta_train_root,\n",
    "                dataset_name=dataset_name,\n",
    "                split_name=split_name,\n",
    "                seq_len=seq_len,\n",
    "                force_override=False,\n",
    "            )\n",
    "\n",
    "            # Extract and save model predictions & ground truth\n",
    "            extract_output_pred_true(\n",
    "                run_configs=run_configs,\n",
    "                dataset_name=dataset_name,\n",
    "                meta_train_root=meta_train_root,\n",
    "                split_name=split_name,\n",
    "                seq_len=seq_len,\n",
    "                label_len=label_len,\n",
    "                pred_len=pred_len,\n",
    "                force_override=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e80f3a",
   "metadata": {},
   "source": [
    "# [4] TimeFuse: Fusor training and evaluation\n",
    "\n",
    "Train the TimeFuse fusor model on the meta-training data.\n",
    "\n",
    "The fusor takes input meta-features `x_meta` and outputs a weight vector `w` with length k for the k base models. The weight vector is used to combine the predictions of the k base models to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf2c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= TIMEFUSE Meta Training Config ==========\n",
      "loss=SmoothL1Loss(), dim_meta_feats=22, dim_model_weights=6\n",
      "n_epochs=5, batch_size=64, learning_rate=0.0005, device=cuda:3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from timefuse import (\n",
    "    ModelFusor,\n",
    "    get_datasets_and_loaders,\n",
    "    get_scaler,\n",
    "    test_fusor,\n",
    "    print_test_scores,\n",
    "    get_length_aligned_loaders,\n",
    ")\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "random_seed = 2021\n",
    "n_epochs = 5  # meta training epochs\n",
    "batch_size = 64  # meta batch size\n",
    "learning_rate = 0.0005  # fusor learning rate\n",
    "num_workers = 1\n",
    "gpu_id = 3  # the gpu id to use for meta training\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "meta_train_data_names = [\n",
    "    f\"{dataname}_val\" for dataname in run_configs[\"datasets\"]\n",
    "]  # for meta training\n",
    "meta_test_data_names = [\n",
    "    f\"{dataname}_test\" for dataname in run_configs[\"datasets\"]\n",
    "]  # for meta testing\n",
    "\n",
    "dim_meta_feats = 22  # fusor input dim\n",
    "dim_model_weights = len(\n",
    "    run_configs[\"models\"]\n",
    ")  # fusor output dim, i.e., number of models\n",
    "\n",
    "meta_scaler = get_scaler(\"standard\")  # input meta feature scaler\n",
    "\n",
    "# Initialize model and optimizer\n",
    "fusor = ModelFusor(input_dim=dim_meta_feats, output_dim=dim_model_weights)\n",
    "fusor.to(device)\n",
    "optimizer = optim.Adam(fusor.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.SmoothL1Loss(beta=0.01)\n",
    "\n",
    "print(\n",
    "    f\" TIMEFUSE Meta Training Config \".center(50, \"=\") + \"\\n\"\n",
    "    f\"loss={criterion}, dim_meta_feats={dim_meta_feats}, dim_model_weights={dim_model_weights}\\n\"\n",
    "    f\"n_epochs={n_epochs}, batch_size={batch_size}, learning_rate={learning_rate}, device={device}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d078a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////// Forecast: [96, 48, 96] //////\n",
      "\n",
      "Array has been loaded from './meta_data/ETTh1_val/x_meta_96.h5' (0.01s)\n",
      "Array has been loaded from './meta_data/ETTh1_val/y_pred_96_48_96.h5' (0.35s)\n",
      "Array has been loaded from './meta_data/ETTh1_val/y_true_96_48_96.h5' (0.04s)\n",
      "Array has been loaded from './meta_data/ETTh2_val/x_meta_96.h5' (0.00s)\n",
      "Array has been loaded from './meta_data/ETTh2_val/y_pred_96_48_96.h5' (0.32s)\n",
      "Array has been loaded from './meta_data/ETTh2_val/y_true_96_48_96.h5' (0.04s)\n",
      "Array has been loaded from './meta_data/ETTh1_test/x_meta_96.h5' (0.00s)\n",
      "Array has been loaded from './meta_data/ETTh1_test/y_pred_96_48_96.h5' (0.32s)\n",
      "Array has been loaded from './meta_data/ETTh1_test/y_true_96_48_96.h5' (0.03s)\n",
      "Array has been loaded from './meta_data/ETTh2_test/x_meta_96.h5' (0.00s)\n",
      "Array has been loaded from './meta_data/ETTh2_test/y_pred_96_48_96.h5' (0.33s)\n",
      "Array has been loaded from './meta_data/ETTh2_test/y_true_96_48_96.h5' (0.03s)\n",
      "Fitting the scaler for the meta-features ... done in 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/5 | meta-train :   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning loaders into iterators ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/5 | meta-train :   2%|▏         | 1/44 [00:00<00:07,  5.51it/s, ETTh1_val=0.92, ETTh2_val=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/5 | meta-train : 100%|██████████| 44/44 [00:00<00:00, 88.87it/s, ETTh1_val=0.92, ETTh2_val=0.333] \n",
      " meta-test  | ETTh2_val : 100%|██████████| 2/2 [00:00<00:00,  3.92it/s]\n",
      " meta-test  | ETTh2_test : 100%|██████████| 2/2 [00:00<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETTh1_test | MSE: 0.3704 (-0.0080) | MAE: 0.3913 (-0.0068) | \n",
      "ETTh2_test | MSE: 0.2765 (-0.0089) | MAE: 0.3322 (-0.0053) | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/5 | meta-train :   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning loaders into iterators ... done in 0.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/5 | meta-train : 100%|██████████| 44/44 [00:00<00:00, 87.10it/s, ETTh1_val=0.853, ETTh2_val=0.333]\n",
      " meta-test  | ETTh2_val : 100%|██████████| 2/2 [00:00<00:00,  4.27it/s]\n",
      " meta-test  | ETTh2_test : 100%|██████████| 2/2 [00:00<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETTh1_test | MSE: 0.3704 (-0.0080) | MAE: 0.3913 (-0.0068) | \n",
      "ETTh2_test | MSE: 0.2765 (-0.0089) | MAE: 0.3322 (-0.0053) | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/5 | meta-train :   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning loaders into iterators ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/5 | meta-train :   2%|▏         | 1/44 [00:00<00:08,  5.25it/s, ETTh1_val=0.91, ETTh2_val=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/5 | meta-train : 100%|██████████| 44/44 [00:00<00:00, 87.72it/s, ETTh1_val=0.91, ETTh2_val=0.316] \n",
      " meta-test  | ETTh2_val : 100%|██████████| 2/2 [00:00<00:00,  4.17it/s]\n",
      " meta-test  | ETTh2_test : 100%|██████████| 2/2 [00:00<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETTh1_test | MSE: 0.3704 (-0.0080) | MAE: 0.3913 (-0.0068) | \n",
      "ETTh2_test | MSE: 0.2766 (-0.0088) | MAE: 0.3322 (-0.0053) | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/5 | meta-train :   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning loaders into iterators ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/5 | meta-train :   2%|▏         | 1/44 [00:00<00:06,  6.48it/s, ETTh1_val=0.851, ETTh2_val=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/5 | meta-train : 100%|██████████| 44/44 [00:00<00:00, 97.02it/s, ETTh1_val=0.851, ETTh2_val=0.339] \n",
      " meta-test  | ETTh2_val : 100%|██████████| 2/2 [00:00<00:00,  4.10it/s]\n",
      " meta-test  | ETTh2_test : 100%|██████████| 2/2 [00:00<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETTh1_test | MSE: 0.3704 (-0.0079) | MAE: 0.3913 (-0.0068) | \n",
      "ETTh2_test | MSE: 0.2766 (-0.0088) | MAE: 0.3323 (-0.0053) | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/5 | meta-train :   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning loaders into iterators ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/5 | meta-train :   2%|▏         | 1/44 [00:00<00:08,  5.37it/s, ETTh1_val=0.863, ETTh2_val=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/5 | meta-train : 100%|██████████| 44/44 [00:00<00:00, 89.51it/s, ETTh1_val=0.863, ETTh2_val=0.321]\n",
      " meta-test  | ETTh2_val : 100%|██████████| 2/2 [00:00<00:00,  4.07it/s]\n",
      " meta-test  | ETTh2_test : 100%|██████████| 2/2 [00:00<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETTh1_test | MSE: 0.3704 (-0.0079) | MAE: 0.3913 (-0.0068) | \n",
      "ETTh2_test | MSE: 0.2767 (-0.0087) | MAE: 0.3323 (-0.0053) | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.metrics import metric, ALL_METRICS\n",
    "\n",
    "\n",
    "for forecast_settings in run_configs[\"forecast_settings\"]:\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    training_step = forecast_settings[2]\n",
    "\n",
    "    print(f\"\\n////// Forecast: {forecast_settings} //////\\n\")\n",
    "\n",
    "    # Initialize data loaders and datasets\n",
    "    dataload_kwargs = {\n",
    "        \"forecast_setting\": forecast_settings,\n",
    "        \"subset_seed\": random_seed,\n",
    "        \"num_workers\": num_workers,\n",
    "    }\n",
    "    meta_train_datasets, meta_train_loaders = get_datasets_and_loaders(\n",
    "        meta_train_data_names,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        **dataload_kwargs,\n",
    "    )\n",
    "    meta_test_datasets, meta_test_loaders = get_datasets_and_loaders(\n",
    "        meta_test_data_names,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        **dataload_kwargs,\n",
    "    )\n",
    "    # Get aligned length loaders\n",
    "    aligned_train_loaders = get_length_aligned_loaders(\n",
    "        meta_train_datasets,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # compute the best single performance for each dataset\n",
    "    model_scores = {}\n",
    "    for data_name, meta_dataset in meta_test_datasets.items():\n",
    "        model_preds = meta_dataset.y_model_preds\n",
    "        true = meta_dataset.y_true\n",
    "        model_scores[data_name] = {}\n",
    "        for model_id, model_name in enumerate(run_configs[\"models\"]):\n",
    "            model_score = metric(\n",
    "                pred=model_preds[:, model_id, :, :],\n",
    "                true=true,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            model_scores[data_name][model_name] = model_score\n",
    "    test_best_single_perf = {}\n",
    "    for data_name, data_scores in model_scores.items():\n",
    "        test_best_single_perf[data_name] = {}\n",
    "        for metric_name in ALL_METRICS:\n",
    "            scores = [v[metric_name] for k, v in data_scores.items()]\n",
    "            best_score = min(scores)\n",
    "            test_best_single_perf[data_name][metric_name] = best_score\n",
    "\n",
    "    # Get the scaler for the meta-features\n",
    "    start_time = time.time()\n",
    "    print(\"Fitting the scaler for the meta-features ... \", end=\"\")\n",
    "    scaler = get_scaler(\"standard\")\n",
    "    all_meta_x = np.concatenate(\n",
    "        [dataset.x_meta for dataset in meta_train_datasets.values()]\n",
    "    )\n",
    "    scaler.fit(all_meta_x)\n",
    "    print(f\"done in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    n_batch = max([len(loader) for loader in aligned_train_loaders.values()])\n",
    "\n",
    "    # Train the model\n",
    "    all_weights = {\n",
    "        \"meta_train\": {data_name: [] for data_name in meta_train_data_names},\n",
    "        \"meta_test\": {data_name: [] for data_name in meta_test_data_names},\n",
    "    }\n",
    "    for i_epoch in range(n_epochs):\n",
    "        prefix = f\"Ep {i_epoch + 1}/{n_epochs}\"\n",
    "        # train over meta-train loaders\n",
    "        train_losses = {data_name: [] for data_name in meta_train_data_names}\n",
    "        # train_weights = {data_name: [] for data_name in meta_train_data_names}\n",
    "        iterator = tqdm.tqdm(\n",
    "            range(n_batch),\n",
    "            total=n_batch,\n",
    "            desc=f\"{prefix} | meta-train \",\n",
    "        )\n",
    "\n",
    "        # turn loaders into iterators\n",
    "        start_time = time.time()\n",
    "        print(\"Turning loaders into iterators ... \", end=\"\")\n",
    "        meta_train_iterators = {\n",
    "            data_name: iter(aligned_train_loaders[data_name])\n",
    "            for data_name in meta_train_data_names\n",
    "        }\n",
    "        print(f\"done in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        for i_batch in iterator:\n",
    "            for train_name, meta_loader in meta_train_iterators.items():\n",
    "                x_meta, y_model_preds, y_true = next(meta_loader)\n",
    "\n",
    "                x_meta = (\n",
    "                    scaler.transform(x_meta)  # Scale the meta-features\n",
    "                    .float()\n",
    "                    .to(device)\n",
    "                )\n",
    "                y_model_preds = y_model_preds.float().to(device)\n",
    "                y_true = y_true.float().to(device)\n",
    "\n",
    "                weights = fusor(x_meta)\n",
    "                # train_weights[train_name].append(weights.detach().cpu().numpy())\n",
    "                # Reshape weights to enable broadcasting with y_model_preds\n",
    "                weights = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "                # Fuse the output by weighting model predictions\n",
    "                # y_model_preds shape: (32, 14, 96, 7)\n",
    "                # Resulting weighted shape: (32, 14, 96, 7)\n",
    "                weighted_preds = weights * y_model_preds\n",
    "\n",
    "                # Sum along the model dimension (dim=1) to get the fused output\n",
    "                # fused_output shape: (32, 96, 7)\n",
    "                fused_output = torch.sum(weighted_preds, dim=1)\n",
    "\n",
    "                # Calculate loss and backpropagate\n",
    "                loss = criterion(fused_output, y_true)\n",
    "                train_loss = criterion(\n",
    "                    fused_output[:, :training_step], y_true[:, :training_step]\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # stop if weights contain NaN\n",
    "                if torch.isnan(weights).any():\n",
    "                    print(\"NaN weight detected\")\n",
    "                    raise ValueError\n",
    "\n",
    "                train_losses[train_name].append(loss.item())\n",
    "\n",
    "            if i_batch % 100 == 0:\n",
    "                info = {\n",
    "                    train_name: np.mean(train_losses[train_name])\n",
    "                    for train_name in train_losses\n",
    "                }\n",
    "                iterator.set_postfix(**info)\n",
    "\n",
    "        # update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # test over meta-test loaders\n",
    "\n",
    "        meta_train_scores, meta_train_weights = test_fusor(\n",
    "            fusor,\n",
    "            scaler,\n",
    "            meta_train_loaders,\n",
    "            device,\n",
    "        )\n",
    "        meta_test_scores, meta_test_weights = test_fusor(\n",
    "            fusor,\n",
    "            scaler,\n",
    "            meta_test_loaders,\n",
    "            device,\n",
    "        )\n",
    "        for data_name in meta_train_loaders.keys():\n",
    "            all_weights[\"meta_train\"][data_name].append(meta_train_weights[data_name])\n",
    "        for data_name in meta_test_loaders.keys():\n",
    "            all_weights[\"meta_test\"][data_name].append(meta_test_weights[data_name])\n",
    "        print_test_scores(meta_test_scores, test_best_single_perf, [\"mse\", \"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e01ffab",
   "metadata": {},
   "source": [
    "# [5] Results Print\n",
    "\n",
    "Finally, we collect the results of TimeFuse and the base models, and print them in a table format.\n",
    "\n",
    "We highlight the best model for each dataset and metric in green, the 2nd best in yellow, and the others in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbcf6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset           Metric            TimeFuse (Ours)   TimeXer           TimeMixer         PAttn             TimesNet          PatchTST          DLinear           \n",
      "ETTh1_test        MSE               \u001b[1;32m0.3704**          \u001b[0m\u001b[1;31m0.3818            \u001b[0m\u001b[1;31m0.3784            \u001b[0m\u001b[1;33m0.3901*           \u001b[0m\u001b[1;31m0.3891            \u001b[0m\u001b[1;31m0.3793            \u001b[0m\u001b[1;31m0.3962            \u001b[0m\n",
      "ETTh1_test        MAE               \u001b[1;32m0.3913**          \u001b[0m\u001b[1;31m0.4029            \u001b[0m\u001b[1;31m0.3981            \u001b[0m\u001b[1;33m0.4048*           \u001b[0m\u001b[1;31m0.4120            \u001b[0m\u001b[1;31m0.3996            \u001b[0m\u001b[1;31m0.4108            \u001b[0m\n",
      "ETTh2_test        MSE               \u001b[1;32m0.2767**          \u001b[0m\u001b[1;33m0.2854*           \u001b[0m\u001b[1;31m0.2901            \u001b[0m\u001b[1;31m0.2988            \u001b[0m\u001b[1;31m0.3370            \u001b[0m\u001b[1;31m0.2921            \u001b[0m\u001b[1;31m0.3414            \u001b[0m\n",
      "ETTh2_test        MAE               \u001b[1;32m0.3323**          \u001b[0m\u001b[1;33m0.3376*           \u001b[0m\u001b[1;31m0.3406            \u001b[0m\u001b[1;31m0.3453            \u001b[0m\u001b[1;31m0.3709            \u001b[0m\u001b[1;31m0.3452            \u001b[0m\u001b[1;31m0.3953            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for data_name in model_scores.keys():  # add TimeFuse scores to model_scores\n",
    "    model_scores[data_name][\"TimeFuse (Ours)\"] = meta_test_scores[data_name]\n",
    "\n",
    "metrics = [\"mse\", \"mae\"]\n",
    "print_models = [\"TimeFuse (Ours)\"] + run_configs[\"models\"][::-1]\n",
    "len_cell = 18\n",
    "\n",
    "info = f\"{'Dataset':<{len_cell}}{'Metric':<{len_cell}}\"\n",
    "for model_name in print_models:\n",
    "    info += f\"{model_name:<{len_cell}}\"\n",
    "print(info)\n",
    "\n",
    "for data_name, data_scores in model_scores.items():\n",
    "    for metric_name in metrics:\n",
    "        info = f\"{data_name:<{len_cell}}{metric_name.upper():<{len_cell}}\"\n",
    "        scores = [data_scores[model_name][metric_name] for model_name in print_models]\n",
    "        model_score_ranks = np.argsort(scores)\n",
    "        for i_model, model_name in enumerate(print_models):\n",
    "            score = f\"{data_scores[model_name][metric_name]:.4f}\"\n",
    "            if model_score_ranks[i_model] == 0:\n",
    "                score = f\"\\033[1;32m{f'{score}**':<{len_cell}}\\033[0m\"  # Best score: green**\n",
    "            elif model_score_ranks[i_model] == 1:\n",
    "                score = f\"\\033[1;33m{f'{score}*':<{len_cell}}\\033[0m\"  # 2nd best score: yellow*\n",
    "            else:\n",
    "                score = f\"\\033[1;31m{score:<{len_cell}}\\033[0m\"  # 3rd best score: red\n",
    "            info += f\"{score:<{len_cell}}\"\n",
    "        print(info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
